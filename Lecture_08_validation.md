# 予測能力の評価について

このシリーズは、Meta-learnersの構造の説明だったので、Base-learnerの最適化についてあまり説明しませんでした。ハイパーパラメータを`rforest`コマンドの初期設定を採用していましたが、これが最適である保証はありません。手順の簡単のために省略しました。

また、[講義4では予測能評価としてデータスプリットを採用しました](https://github.com/sankyoh/Stata_Lecture2023/blob/main/Lecture_04_prediction2.md#%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88%E3%82%92%E5%88%86%E5%89%B2%E3%81%99%E3%82%8B)。
この目的は説明を簡単にするためであり、実際にはあまり推奨されない手法です（が、簡単なので良く使われているようです）。

では、どうするのか？という説明を最終回として加えたいと思います。

## Apparent performance

モデルを構築するときに使ったデータセットで予測能を評価すると、当然に過剰に予測能力は高くなります（過学習/オーバーフィッティングのため）。この時の予測能をApparent perfoemanceと言い、この減少をoptimismと言います。
ただし、非常に大規模なサンプルサイズであれば、optimismを無視できるようになります。

サンプルサイズが十分で無いにもかかわらず、このApparent perfoemanceを真の予測能とすると良いことにはなりません。

## 対応方法
統計モデル（機械学習モデル）の予測能力は、その評価方法によって大きく異なります。ここでは内部検証と外部検証について紹介します。

# 内的検証
内部検証は、予測モデル開発に使用したのと同じデータを評価に使用する方法です。

## データスプリット
データスプリットは、[講義4](https://github.com/sankyoh/Stata_Lecture2023/blob/main/Lecture_04_prediction2.md)で採用した方法です。

やり方は、とてもシンプルですし、Stataでの実装もそう複雑ではありません。。

しかし、これには既知の欠点があります。
* 学習用データが少なくなり、オーバーフィッティングする。
* 検証用データが少なくなり、うまく検証できない。
* 良い感じの評価になるようにデータスプリットを繰り返すズル（Cherry picking）ができる。

そのため、論文の主解析などには含めない方が良いと思います（Supplementに加えるくらいは良い様に思いますが）。

## k-fold cross validation（k分割交差検証）
全てのデータを用いることができるのが利点です。おそらく最も多く用いられている方法です。Pythonだと（多分、Rも）交差検証を行う関数があります。Stataでは`lasso`のパラメータを決める時に交差検証が利用されていますが、広く一般に利用できるものは無いようです。

次の様な手順をとります。
1. ランダムにデータをk分割する（下図ではk=4）。
2. 予測モデル構築とその評価をk回繰り返す。
3. 下図のように青色サブセットで予測モデル構築を行う。
4. 黄色サブセットで構築された予測モデルの評価を行う。
5. 評価指標の平均値をとる。

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/2fed1e02-825b-477d-ba67-f8ac325b31aa)

## ブートストラップ標本を用いた評価
### ブートストラップ標本とは何か
ブートストラップは、サンプルサイズＮのデータセットについて、重複を許してＮ回リサンプリングする方法です。

たとえば、15人の身長データがあり、それぞれの人の値が下表のようだったとします。平均値=174.53cmです。

| 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   | 9   | 10  | 11  | 12  | 13  | 14  | 15  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 170 | 170 | 172 | 172 | 173 | 173 | 173 | 173 | 173 | 173 | 178 | 178 | 178 | 181 | 181 |

重複を許してリサンプリングを行うと、15人それぞれが1/15の確率でサンプリングされます。

つまり、1回のリサンプリングでは下記の確率でサンプリングされます。
* 2/15の確率で「170」の人
* 2/15の確率で「172」の人
* 6/15の確率で「173」の人
* 3/15の確率で「178」の人
* 2/15の確率で「181」の人

サンプルサイズが15なので、15回繰り返すと1つの**ブートストラップ標本**が得られます。平均値174.73cmです（元データセットとほぼ同じです）。

例えば、次のようなブートストラップ標本が得られます。
| 1   | 1   | 4   | 4   | 5   | 5   | 5   | 8   | 8   | 10  | 11  | 11  | 14  | 14  | 15  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 170 | 170 | 172 | 172 | 173 | 173 | 173 | 173 | 173 | 173 | 178 | 178 | 181 | 181 | 181 |

### ブートストラップ標本を用いて評価する。
このようにして作成したブートストラップ標本を評価用のデータとして使って、モデルの予測能を評価することができます。

ただし、1つのブートストラップ標本では、偶然の偏りが生じることがありますので、複数回繰り返します（1000回とか）。

次の様な手順をとります。
1. 元データを用いて予測モデルを構築し、元データのApparent performanceを求める。
2. ブートストラップ標本を作り、1と同様にして予測モデルを構築する。
3. 手順2の予測モデルをブートストラップ標本で評価する（ブートストラップ標本のApparent performancewを求める）。
4. 手順2の予測モデルを元データセットで評価する。
5. 手順3と4の差分をoptimismとする。
6. この手順を繰り返し、optmismの平均を求める。
7. 手順1で求めたApparent performanceから手順6で求めた平均optimismを加え/差し引いて、最終的な性能として評価する。

手順7の「加え/差し引いて」は、評価基準が低い方が良い場合（エラーなど）は加え、高い方が良い場合（AUCなど）は差し引きます。

## Out-of-Bag
`rforest`コマンドの説明の際にも紹介したものです。ランダムフォレストを実行するStata外部コマンド`rforest`では、「ブートストラップ標本を生成し、それに対して決定木分析を行う」という操作を繰り返します。

このときに、個々の決定木分析では元標本には存在するけれど、ブートストラップ標本には選ばれなかった標本が存在します。これを **Out-of-Bag(OOB)** と言います。

上記の例では、
1, 4, 5, 7, 8, 10, 11, 14, 15
がブートストラップ標本として選ばれていますので、
 **2, 3, 6, 9, 12, 13**
がOOBです。

OOBを使う方法は、
1. ブートストラップ標本で学習する。
2. OOBで評価する（**OOBエラー**を算出する）。
という方法です。

ランダムフォレストは、決定木分析を1000回とか複数回行いますので、OOBエラーも複数個計算されます。それらの平均値を予測モデルの評価として用います。

`rforest`コマンドでは、ランダムフォレスト実行後に`di e(OOB_Error)`で表示させることができます。

## 内的-外的交差検証
データセットが元々クラスタ別に分けられるような場合を想定しています。

データセットが日本全国のもので、都道府県を示す変数があれば、k-fold cross validationと同じように検証します（ここでは、例としてクラスタ＝都道府県としました）。

例えば、北海道以外の46都府県で予測モデルを構築し、その予測モデルを北海道に適応し、予測能を評価します。次に、青森県以外の46都道府県で予測能を構築し、その予測モデルを青森県に適応し、予測能を評価します。これを沖縄県まで繰り返し、47個の評価指標の平均をとるという方法です。

この単位は都道府県ではなく、市町村でも良いですし、医療機関などの単位でもＯＫです。

この方法では、極端な地域におけるモデル性能に異質性がでることが予想されます。都道府県の例では、北海道と沖縄県で評価した場合は、予測能が低いでしょう。これは「それが悪い」ということではなく、どのようなクラスタで予測能が低くなるかを知ることで、異質性の原因や、予測能を高めるための戦略を考察する一助とすることができます。

## 他にもいろいろ


# 外的検証
モデル構築とは全く異なる新しいデータセットで、モデル性能を推定する事です。

いくつかの種類（興味の対象）があることに注意が必要です。

## 再現性を評価する
過学習が起こっていないことを確認するために、予想モデル構築と「同じ母集団から抽出された違うデータセット」での予測能を評価します。この「同じ母集団から抽出された違うデータセット」での結果が悪いようなら、過学習が起こっている可能性が懸念されます。

例えば岡山市のデータセットＡを用いて予想モデルを構築し、内部検証を行った場合に、岡山市のデータセットＢを用いて外部検証を行うようなイメージです。

## Transportabilityの評価
予測モデル構築を行ったのとは「異なる母集団から抽出されたデータセット」での予測能を評価します。得られた予測モデルが「他の集団にも適応可能か？」ということが興味の中心です。

例えば、那覇市のデータセットを用いて予測モデルを構築し、内部検証を行った場合に、札幌市のデータセットを用いて外部検証を行うようなイメージです。この例では（地理的条件の違いにより）Transportabilityは無いと評価されると思います。

一方で、岡山市データで予測モデルを構築し、内部検証を行った場合に、倉敷市のデータセットを用いて外部検証を行ったならば、Transportabilityはあると評価されると思います。これは地理的条件がほぼ同じだからです。

## 外的検証は本当に必要か？
論文で予測モデルを構築すると、「外的検証を行ったか？」という査読者コメントが出されることがあります。

これに回答する時には、まず「外的検証を行うのにふさわしいデータがあるのか？」という問いにこたえなければなりません。この問いが「いいえ」であれば、外的検証は行えません。便宜的に入手できる、それっぽいデータを用いて「外的検証もどき」を行うことは不適切です。

この問いに「はい」と答えられるのであれば、適切な方法で外部検証を行うべきですが、そういう事態はあまり多くはないでしょう。

よくやられているのは、次の様な方法があります。

データを2つに分ける。80%：20%くらい。80%の方で、モデル構築＋内部検証を行う。内部検証の過程でモデルの洗練を行う（内部検証における予測能を最大化させる）。そのモデルで残り20%に適応して、予測能を評価し、外部検証とする。

これは機械学習（画像の判定など）でもよく行われています。

# Stataでやってみる。
では、内部検証についていくつかStataでもやってみようと思います。

## データの読込み
いつものデータを読み込みます。

```
* データ読み込み
use https://www.stata-press.com/data/r18/cattaneo2, clear

* IDを作っておく
gen id = _n

* 変数リストの定義
vl set
vl create xcate = (mmarried mhisp fhisp foreign alcohol deadkids mrace frace prenatal fbaby prenatal1 order birthmonth)
vl create xcont = (monthslb mage medu fage fedu nprenatal)
```

ここでは説明が目的であり予測モデルを具体的に構築するわけではないので、下記の回帰分析モデルの評価をします。また、評価指標はRMSEとします。

```
regress bweight $xcate $xcont
```

$$RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2}$$

## スプリット
講義4で行ったのと基本的に同じ事をします。

まず、`splitsample`を用いてデータセットを学習データと検証データに分割します。

```
splitsample, generate(svar, replace) split(0.2 0.8) show rseed(12345)
```

ここで、良い感じの結果になるまで、`rseed(12345)`を代えて粘るということが可能ですが、それはズル（Cherry picking）です。止めましょう。

ランダムシードは明らかに他意がないものにしましょう。12345とか、3141とか、1111とか、19271105(赤池弘次先生の誕生日)とかが良いでしょう。

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/b97248ff-38fd-4d7f-a987-1628f3f54535)

では、予測モデルを構築し、評価を行います。

```
* 学習データで予測モデル構築
regress bweight $xcate $xcont if svar==2

* 寄り道せいて、Apparent RMSEも計算する。
predict resi_appa, residual
gen     resi_appa2 = resi_appa^2
su resi_appa2
local rmse_appa = sqrt(`r(mean)')

* 検証データでRMSEを算出する
predict resi_split, residual
gen     resi_split2 = resi_split^2 if svar==1
su resi_split2 if svar==1
local rmse_split = sqrt(`r(mean)')

* 計算結果を表示する。
display "RMSE of Apparent = `rmse_apparent'"
display "RMSE of Split    = `rmse_split'"
```

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/8eb751f7-fd53-419a-a253-329b39513d3e)

Apparent RMSE = 550, Data split RMSE = 571となっています。当然ですが、Apparentで低い数値担っています。

## k-fold cross validation
次に10分割交差検証を行います。

まず、10分割を行います。

```
splitsample, generate(cv, replace) nsplit(10) show rseed(12345)
```

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/acee36f5-7d5e-43a4-a15a-e45ca23f805e)

この10分割されたデータセットの9つを使って予測モデルを構築し、残りの1つで検証していきます。

```
* 予測モデル構築+検証を10回行う。
local rmse_cv = 0
forvalues x=1/10 {
	* x番目以外のサブセットで予測モデルを構築
	qui:regress bweight $xcate $xcont if cv!=`x'
	
	* 検証データでRMSEを算出する
	predict resi_cv`x', residual
	gen     resi_cv`x'_2 = resi_cv`x'^2 if cv==`x'
	su resi_cv`x'_2 if cv==`x'
	local rmse_cv`x' = sqrt(`r(mean)')
	local rmse_cv = `rmse_cv' + `rmse_cv`x''
	di "`rmse_cv`x''"
}
local rmse_cv = `rmse_cv'/10

* 計算結果を表示する。
display "RMSE of Apparent = `rmse_appa'"
display "RMSE of Split    = `rmse_split'"
display "RMSE of 10f-CV   = `rmse_cv'"
```

次の様な結果になります。

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/7562ccb8-f9df-4bf0-be9d-2c0cf9ad704d)

## ブートストラップによる方法

まず、手順を思い出しましょう。
1. 元データを用いて予測モデルを構築し、元データのApparent performanceを求める。
2. ブートストラップ標本を作り、1と同様にして予測モデルを構築する。
3. 手順2の予測モデルをブートストラップ標本で評価する（ブートストラップ標本のApparent performancewを求める）。
4. 手順2の予測モデルを元データセットで評価する。
5. 手順3と4の差分をoptimismとする。
6. この手順を繰り返し、optmismの平均を求める。
7. 手順1で求めたApparent performanceから手順6で求めた平均optimismを加え/差し引いて、最終的な性能として評価する。

手順1はもう終わっているので、割愛します（Apparent perfoemance = 550.9）。  

```
set seed 12345
local optism = 0
local bloop =1000
forvalues x=1/`bloop' {
	* オリジナルデータをメモリに保存してからブートストラップサンプリングする。
	preserve
	bsample
	
	* ブートストラップ標本で予測モデルを構築する。
	qui:regress bweight $xcate $xcont
	
	* ブートストラップ標本でのApparent RMSE
	predict resi_bappa, residual
	gen     resi_bappa2 = resi_bappa^2
	su resi_bappa2
	local rmse_bappa = sqrt(`r(mean)')
	di "RMSE bappa = `rmse_bappa'"
	
	* オリジナルデータセットに戻す
	restore
	
	* オリジナル標本でのRMSE
	cap drop resi_boot resi_boot2
	predict resi_boot, residual
	gen     resi_boot2 = resi_boot^2
	su resi_boot2
	local rmse_boot = sqrt(`r(mean)')
	di "RMSE boot= `rmse_boot'"
	
	* Optismの計算
	local optism`x' = `rmse_boot' - `rmse_bappa'
	di "Optism = `optism`x''"
	local optism = `optism' + `optism`x''
}

* 最終的なoptismとrmseの計算
local optism = `optism'/`bloop'
local rmse_bootstrap = `rmse_appa' + `optism'

* 計算結果を表示する。
display "RMSE of Apparent  = `rmse_appa'"
display "RMSE of Split     = `rmse_split'"
display "RMSE of 10f-CV    = `rmse_cv'"
display "RMSE of Bootstarp = `rmse_bootstrap'"
```

結果として次の様になりました。

![image](https://github.com/sankyoh/Stata_Lecture2023/assets/67684585/d491a9a1-d25a-4281-9c45-ef181937d52d)

## 実装した4つのRMSEからわかること。
まず、データスプリットしたRMSEだけ大きく外れていることがわかります。

乱数によってデータスプリットしていますが、偏りが生じたのかもしれません。このため、データスプリットによる内部検証は必ずしも最善の選択ではありません。

次に、Apparentと10-fold CV（10分割交差検証）とブートストラップ法による内部検証は近い値になっています。今回のサンプルサイズが4000以上であり、大きいため、Apparentではあまり影響を受けなかったという事が考えられます。
